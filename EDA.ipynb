{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1c07ff9"
      },
      "source": [
        "### Import Libraries\n",
        "\n",
        "This cell imports all the necessary libraries for data manipulation, machine learning models, and evaluation."
      ],
      "id": "e1c07ff9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02614695",
      "metadata": {
        "id": "02614695"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import xgboost as xgb\n",
        "import shap\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    import zipfile\n",
        "    import io\n",
        "\n",
        "    uploaded_file_name = '/content/Airbnb-price-predictor.zip' # Replace with the actual uploaded file name\n",
        "    with zipfile.ZipFile(uploaded_file_name, 'r') as zf:\n",
        "        zf.extractall('/content/')\n"
      ],
      "metadata": {
        "id": "EK7Ep8RRV5Vz"
      },
      "id": "EK7Ep8RRV5Vz",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ba0f5c3"
      },
      "source": [
        "### Concatenate Big Cities DataFrames\n",
        "\n",
        "This cell combines the DataFrames of big cities (NYC, Chicago, LA, SF) into a single DataFrame named `big_cities` and prints the number of columns in the resulting DataFrame."
      ],
      "id": "7ba0f5c3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad61500f"
      },
      "source": [
        "### Display Shape of `big_cities`\n",
        "\n",
        "This cell shows the dimensions (rows, columns) of the `big_cities` DataFrame after concatenation."
      ],
      "id": "ad61500f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64ea4e9f"
      },
      "source": [
        "### Display Shapes of Medium Cities DataFrames\n",
        "\n",
        "This cell defines a list of medium cities DataFrames and then iterates through them to print the shape of each DataFrame."
      ],
      "id": "64ea4e9f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af9fb687"
      },
      "source": [
        "### Concatenate Medium Cities DataFrames\n",
        "\n",
        "This cell combines the DataFrames of medium cities (Denver, Portland, Austin, Seattle) into a single DataFrame named `medium_cities`."
      ],
      "id": "af9fb687"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5abb28e5"
      },
      "source": [
        "### Display Shape of `medium_cities`\n",
        "\n",
        "This cell shows the dimensions (rows, columns) of the `medium_cities` DataFrame after concatenation."
      ],
      "id": "5abb28e5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51cfdee6"
      },
      "source": [
        "### Display Shapes of Small Cities DataFrames\n",
        "\n",
        "This cell defines a list of small cities DataFrames and then iterates through them to print the shape of each DataFrame."
      ],
      "id": "51cfdee6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbfa2b1c"
      },
      "source": [
        "### Concatenate Small Cities DataFrames\n",
        "\n",
        "This cell combines the DataFrames of small cities (Asheville, Salem, Columbus, Santacruz) into a single DataFrame named `small_cities`."
      ],
      "id": "bbfa2b1c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbbf7105"
      },
      "source": [
        "### Display Shape of `small_cities`\n",
        "\n",
        "This cell shows the dimensions (rows, columns) of the `small_cities` DataFrame after concatenation."
      ],
      "id": "fbbf7105"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9bf85b5"
      },
      "source": [
        "### Define Base Numeric Columns and All Features\n",
        "\n",
        "This cell defines the list of base numerical columns and constructs the `ALL_FEATURES` list, which includes engineered features, and specifies the `TARGET_COL` (price)."
      ],
      "id": "e9bf85b5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abe294b8"
      },
      "source": [
        "### Data Cleaning and Feature Engineering Functions\n",
        "\n",
        "This cell defines several functions:\n",
        "- `parse_price`: Cleans and converts price strings to floats.\n",
        "- `parse_bathrooms`: Extracts and converts bathroom text to numeric values.\n",
        "- `create_features_and_clean`: Applies the parsing functions, handles missing values, creates `log_price`, `amenities_count`, `bath_bed_ratio`, `is_entire_home`, and `avg_sub_review_score` features, and filters the DataFrame to include only the relevant columns."
      ],
      "id": "abe294b8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "459b3ead"
      },
      "source": [
        "### Model Definitions\n",
        "\n",
        "This cell defines functions to create and configure different machine learning models:\n",
        "- `get_xgb_model`: Returns a pre-configured XGBoost Regressor model.\n",
        "- `get_nn_model_arch1`: Returns a Sequential Keras model with a deeper MLP architecture and Dropout regularization.\n",
        "- `get_nn_model_arch2`: Returns a Sequential Keras model with a wider MLP architecture and L2 regularization."
      ],
      "id": "459b3ead"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20226c55"
      },
      "source": [
        "### Model Evaluation and Training Function\n",
        "\n",
        "This cell defines two key functions:\n",
        "- `evaluate_model`: Calculates RMSE, MAE, and R2 scores for a given model and test set.\n",
        "- `train_and_evaluate_composite`: Orchestrates the entire training and evaluation process for a given DataFrame. It performs data preparation, scaling, trains XGBoost and two Neural Network architectures, and stores their results, trained models, and scaled test data."
      ],
      "id": "20226c55"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38d20709"
      },
      "source": [
        "### Execute Training and Evaluation for Composite DataFrames\n",
        "\n",
        "This cell executes the `train_and_evaluate_composite` function for each of the composite DataFrames (Big, Medium, Small cities). It stores the evaluation results, trained models, and test data for each composite, printing status messages during execution."
      ],
      "id": "38d20709"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b29834fc"
      },
      "source": [
        "### Load City DataFrames\n",
        "\n",
        "This cell loads the `listings.csv` file for each specified city into individual pandas DataFrames. These DataFrames will be used for further analysis and model training."
      ],
      "id": "b29834fc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42c22005",
      "metadata": {
        "id": "42c22005"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 1. Asheville\n",
        "df_asheville = pd.read_csv('./data/Asheville/listings.csv')\n",
        "\n",
        "# 2. Austin\n",
        "df_austin = pd.read_csv('./data/Austin/listings.csv')\n",
        "\n",
        "# 3. Chicago\n",
        "df_chicago = pd.read_csv('./data/Chicago/listings.csv')\n",
        "\n",
        "# 4. Columbus\n",
        "df_columbus = pd.read_csv('./data/Columbus/listings.csv')\n",
        "\n",
        "# 5. Denver\n",
        "df_denver = pd.read_csv('./data/Denver/listings.csv')\n",
        "\n",
        "# 6. LA\n",
        "df_la = pd.read_csv('./data/LA/listings.csv')\n",
        "\n",
        "# 7. NYC\n",
        "df_nyc = pd.read_csv('./data/NYC/listings.csv')\n",
        "\n",
        "# 8. Portland\n",
        "df_portland = pd.read_csv('./data/Portland/listings.csv')\n",
        "\n",
        "# 9. Salem\n",
        "df_salem = pd.read_csv('./data/Salem/listings.csv')\n",
        "\n",
        "# 10. Santacruz\n",
        "df_santacruz = pd.read_csv('./data/Santacruz/listings.csv')\n",
        "\n",
        "# 11. Seattle\n",
        "df_seattle = pd.read_csv('./data/Seattle/listings.csv')\n",
        "\n",
        "# 12. SF\n",
        "df_sf = pd.read_csv('./data/SF/listings.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bac26198"
      },
      "source": [
        "### Display Shape of `df_nyc`\n",
        "\n",
        "This cell shows the dimensions (rows, columns) of the `df_nyc` DataFrame."
      ],
      "id": "bac26198"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29eb0683",
      "metadata": {
        "id": "29eb0683",
        "outputId": "2e5f8fb6-7045-4c86-90ba-8148ea3b8d04"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(36111, 79)"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_nyc.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ad4d7ef"
      },
      "source": [
        "### Display Shape of `df_la`\n",
        "\n",
        "This cell shows the dimensions (rows, columns) of the `df_la` DataFrame."
      ],
      "id": "5ad4d7ef"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7c20a6a",
      "metadata": {
        "id": "b7c20a6a",
        "outputId": "26357830-4775-4d53-f8be-3b9ede4d7f23"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(45886, 79)"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_la.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fce7c687"
      },
      "source": [
        "### Display Shape of `df_sf`\n",
        "\n",
        "This cell shows the dimensions (rows, columns) of the `df_sf` DataFrame."
      ],
      "id": "fce7c687"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e179c65b",
      "metadata": {
        "id": "e179c65b",
        "outputId": "26c97a70-7fda-4989-8da9-b38afa289bbe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(7780, 79)"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_sf.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a45d4fe"
      },
      "source": [
        "### Display Shape of `df_chicago`\n",
        "\n",
        "This cell shows the dimensions (rows, columns) of the `df_chicago` DataFrame."
      ],
      "id": "3a45d4fe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a5c8a27",
      "metadata": {
        "id": "1a5c8a27",
        "outputId": "08c907f0-d087-4d87-c7f9-db91f7e9cb16"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(8604, 79)"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_chicago.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4182db1a",
      "metadata": {
        "id": "4182db1a",
        "outputId": "7a85adef-4857-4d4d-e4ca-3084215ff062"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "79"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "big_cities = pd.concat([df_nyc , df_chicago , df_la , df_sf])\n",
        "len(big_cities.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19870b53",
      "metadata": {
        "id": "19870b53",
        "outputId": "846550e1-c3d5-4c97-c172-47e285deed4d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(98381, 79)"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "big_cities.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f58e429c",
      "metadata": {
        "id": "f58e429c",
        "outputId": "012ed343-c86a-4cfe-ab25-8ca163bf9104"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " the shape of the df of is (4910, 79)\n",
            " the shape of the df of is (4425, 79)\n",
            " the shape of the df of is (15187, 79)\n",
            " the shape of the df of is (6996, 79)\n"
          ]
        }
      ],
      "source": [
        "medium_cities_list = [df_denver , df_portland , df_austin, df_seattle]\n",
        "\n",
        "for city in medium_cities_list:\n",
        "    print(f\" the shape of the df of is {city.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5905aaaf",
      "metadata": {
        "id": "5905aaaf"
      },
      "outputs": [],
      "source": [
        "medium_cities = pd.concat(medium_cities_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4dcff8c7",
      "metadata": {
        "id": "4dcff8c7",
        "outputId": "250f6a32-201a-401a-e68f-505be554e897"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(31518, 79)"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "medium_cities.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bd4b35b",
      "metadata": {
        "id": "9bd4b35b",
        "outputId": "8798e99e-7059-49cc-aa95-366a5d62fbba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " the shape of the df of is (2876, 79)\n",
            " the shape of the df of is (351, 79)\n",
            " the shape of the df of is (2877, 79)\n",
            " the shape of the df of is (1739, 79)\n"
          ]
        }
      ],
      "source": [
        "small_cities_list = [df_asheville , df_salem, df_columbus, df_santacruz]\n",
        "\n",
        "for city in small_cities_list:\n",
        "    print(f\" the shape of the df of is {city.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "950aaec0",
      "metadata": {
        "id": "950aaec0"
      },
      "outputs": [],
      "source": [
        "small_cities = pd.concat(small_cities_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c48334e0",
      "metadata": {
        "id": "c48334e0",
        "outputId": "b70c280b-b187-40b6-ccbf-d51ea75ece17"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(7843, 79)"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "small_cities.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ed9b37c",
      "metadata": {
        "id": "5ed9b37c"
      },
      "source": [
        "# Base Numerical Columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89472a43",
      "metadata": {
        "id": "89472a43"
      },
      "outputs": [],
      "source": [
        "\n",
        "BASE_NUMERIC_COLS = [\n",
        "    'accommodates', 'bedrooms', 'beds',\n",
        "    'review_scores_rating', 'review_scores_accuracy',\n",
        "    'review_scores_cleanliness', 'review_scores_checkin',\n",
        "    'review_scores_communication', 'review_scores_location',\n",
        "    'review_scores_value', 'number_of_reviews',\n",
        "    'availability_365', 'minimum_nights', 'maximum_nights'\n",
        "]\n",
        "ALL_FEATURES = BASE_NUMERIC_COLS + ['bathrooms', 'amenities_count', 'bath_bed_ratio', 'is_entire_home', 'avg_sub_review_score']\n",
        "TARGET_COL = 'price'\n",
        "n_features = len(ALL_FEATURES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "002f8ff5",
      "metadata": {
        "id": "002f8ff5"
      },
      "outputs": [],
      "source": [
        "def parse_price(price_str):\n",
        "    if pd.isna(price_str): return np.nan\n",
        "    clean_price = str(price_str).replace('$', '').replace(',', '')\n",
        "    try: return float(clean_price)\n",
        "    except: return np.nan\n",
        "def parse_bathrooms(text):\n",
        "    if pd.isna(text): return 0.0\n",
        "    match = re.search(r\"(\\d+(\\.\\d+)?)\", str(text))\n",
        "    if match: return float(match.group(1))\n",
        "    if 'half-bath' in str(text).lower(): return 0.5\n",
        "    return 0.0\n",
        "def create_features_and_clean(df, tier_name):\n",
        "    df['price'] = df[TARGET_COL].apply(parse_price)\n",
        "    df['bathrooms'] = df['bathrooms_text'].apply(parse_bathrooms)\n",
        "    df = df.dropna(subset=[TARGET_COL])\n",
        "    df = df[df[TARGET_COL] > 10]\n",
        "    df['log_price'] = np.log1p(df[TARGET_COL])\n",
        "    for col in ['bedrooms', 'beds', 'accommodates', 'bathrooms']:\n",
        "        df[col] = df[col].fillna(df[col].median())\n",
        "    review_cols = [c for c in BASE_NUMERIC_COLS if 'review' in c]\n",
        "    for col in review_cols:\n",
        "        df[col] = df[col].fillna(df[col].mean())\n",
        "    df['amenities_count'] = df['amenities'].apply(lambda x: len(str(x).split(',')) if pd.notna(x) else 0)\n",
        "    df['bath_bed_ratio'] = np.where(df['bedrooms'] > 0, df['bathrooms'] / df['bedrooms'], 0)\n",
        "    df['bath_bed_ratio'] = df['bath_bed_ratio'].replace([np.inf, -np.inf], 0).fillna(0)\n",
        "    df['is_entire_home'] = (df['room_type'] == 'Entire home/apt').astype(int)\n",
        "    sub_scores = ['review_scores_accuracy', 'review_scores_cleanliness',\n",
        "                  'review_scores_checkin', 'review_scores_communication',\n",
        "                  'review_scores_location', 'review_scores_value']\n",
        "    df['avg_sub_review_score'] = df[sub_scores].mean(axis=1)\n",
        "    return df.filter(items=ALL_FEATURES + ['log_price'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "618ea91a",
      "metadata": {
        "id": "618ea91a"
      },
      "outputs": [],
      "source": [
        "# MODEL DEFINITION\n",
        "def get_xgb_model(random_state=42):\n",
        "    return xgb.XGBRegressor(\n",
        "        objective='reg:squarederror', n_estimators=100, learning_rate=0.1,\n",
        "        max_depth=6, random_state=random_state)\n",
        "\n",
        "# NN Architecture 1 (Deeper MLP with Dropout regularization)\n",
        "def get_nn_model_arch1(n_features):\n",
        "    model = Sequential([\n",
        "        Dense(128, activation='relu', input_shape=(n_features,)),\n",
        "        Dropout(0.1),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(1)])\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "    return model\n",
        "\n",
        "# NN Architecture 2 (Wider MLP with L2 Regularization)\n",
        "def get_nn_model_arch2(n_features):\n",
        "    model = Sequential([\n",
        "        Dense(256, activation='relu', input_shape=(n_features,)),\n",
        "        Dense(128, activation='relu', kernel_regularizer=keras.regularizers.l2(0.01)),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(1)])\n",
        "    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1635ebf",
      "metadata": {
        "id": "e1635ebf"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, X_test, y_test, is_keras=False):\n",
        "    if is_keras:\n",
        "        _, mae = model.evaluate(X_test, y_test, verbose=0)\n",
        "        y_pred = model.predict(X_test).flatten()\n",
        "    else:\n",
        "        y_pred = model.predict(X_test)\n",
        "        mae = mean_absolute_error(y_test, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    return {'RMSE': rmse, 'MAE': mae, 'R2': r2}\n",
        "\n",
        "def train_and_evaluate_composite(df, tier_name):\n",
        "    # Data Preparation and Scaling (essential for NN)\n",
        "    df_clean = create_features_and_clean(df, tier_name).dropna()\n",
        "    X = df_clean[ALL_FEATURES].values\n",
        "    y = df_clean['log_price'].values\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    results = {}\n",
        "    trained_models = {}\n",
        "\n",
        "    # XGBoost\n",
        "    xgb_model = get_xgb_model()\n",
        "    xgb_model.fit(X_train_scaled, y_train)\n",
        "    results['XGBoost'] = evaluate_model(xgb_model, X_test_scaled, y_test)\n",
        "    trained_models['XGBoost'] = xgb_model\n",
        "\n",
        "    # NN Architecture 1\n",
        "    nn1_model = get_nn_model_arch1(n_features)\n",
        "    nn1_model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, verbose=0)\n",
        "    results['NN_Arch1'] = evaluate_model(nn1_model, X_test_scaled, y_test, is_keras=True)\n",
        "    trained_models['NN_Arch1'] = nn1_model\n",
        "\n",
        "    # NN Architecture 2\n",
        "    nn2_model = get_nn_model_arch2(n_features)\n",
        "    nn2_model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, verbose=0)\n",
        "    results['NN_Arch2'] = evaluate_model(nn2_model, X_test_scaled, y_test, is_keras=True)\n",
        "    trained_models['NN_Arch2'] = nn2_model\n",
        "\n",
        "    return results, trained_models, X_test_scaled, y_test, scaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbce4d9a",
      "metadata": {
        "id": "dbce4d9a",
        "outputId": "38325630-bbb5-4253-d031-5ed83e52298c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training for Big_Composite...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\m62\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:95: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 305us/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\m62\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:95: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 334us/step\n",
            "Finished Big_Composite.\n",
            "Starting training for Medium_Composite...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\m62\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:95: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 413us/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\m62\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:95: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 449us/step\n",
            "Finished Medium_Composite.\n",
            "Starting training for Small_Composite...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\m62\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:95: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 817us/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\m62\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:95: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 813us/step\n",
            "Finished Small_Composite.\n"
          ]
        }
      ],
      "source": [
        "# EXECUTION\n",
        "\n",
        "composite_dfs = {\n",
        "    'Big_Composite': globals().get('big_cities'),\n",
        "    'Medium_Composite': globals().get('medium_cities'),\n",
        "    'Small_Composite': globals().get('small_cities')\n",
        "}\n",
        "\n",
        "composite_results = {}\n",
        "composite_models = {}\n",
        "composite_test_data = {}\n",
        "\n",
        "for name, df in composite_dfs.items():\n",
        "    if df is not None:\n",
        "        print(f\"Starting training for {name}...\")\n",
        "        results, models, X_test, y_test, scaler = train_and_evaluate_composite(df, name.split('_')[0])\n",
        "\n",
        "        composite_results[name] = results\n",
        "        composite_models[name] = models\n",
        "\n",
        "        # Store test data and scaler for Phase 3 (Cross-Tier Analysis)\n",
        "        composite_test_data[name] = {'X_test': X_test, 'y_test': y_test, 'scaler': scaler}\n",
        "        print(f\"Finished {name}.\")\n",
        "    else:\n",
        "        print(f\"Skipping {name}: DataFrame not found. Please ensure it's loaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0e6b7c3",
      "metadata": {
        "id": "a0e6b7c3"
      },
      "source": [
        "# Cross-Tier Neural Network Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "345d5560"
      },
      "source": [
        "### Cross-Tier Analysis Function and Execution\n",
        "\n",
        "This cell defines and executes the `perform_cross_tier_analysis` function. This function evaluates how well models trained on one city tier perform when predicting prices in other city tiers. It then displays the R2 scores from this cross-tier analysis in a formatted Markdown table."
      ],
      "id": "345d5560"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac872aee",
      "metadata": {
        "id": "ac872aee",
        "outputId": "6ad606e6-4c52-498b-fd05-a41693fdaee5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "Phase 3 Complete: Cross-Tier NN Generalization Analysis (R-Squared)\n",
            "======================================================================\n",
            "|                                     | R2      |\n",
            "|:------------------------------------|:--------|\n",
            "| Trained on Big | Tested on Medium   | -1.4242 |\n",
            "| Trained on Big | Tested on Small    | -1.4058 |\n",
            "| Trained on Medium | Tested on Big   | 0.4128  |\n",
            "| Trained on Medium | Tested on Small | 0.5059  |\n",
            "| Trained on Small | Tested on Big    | 0.2362  |\n",
            "| Trained on Small | Tested on Medium | 0.4244  |\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "def evaluate_model(model, X_test, y_test, is_keras=False):\n",
        "    \"\"\"Calculates RMSE, MAE, and R2 for a given model and test set.\"\"\"\n",
        "    if is_keras:\n",
        "        # Suppressing detailed Keras output during prediction\n",
        "        y_pred = model.predict(X_test, verbose=0).flatten()\n",
        "    else:\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    return {'RMSE': rmse, 'MAE': mae, 'R2': r2}\n",
        "\n",
        "def perform_cross_tier_analysis(composite_models, composite_test_data, model_type='NN_Arch1'):\n",
        "    \"\"\"\n",
        "    Evaluates how well models trained on one composite tier predict prices\n",
        "    in the other two tiers.\n",
        "    \"\"\"\n",
        "    cross_tier_results = {}\n",
        "    tier_names = ['Big', 'Medium', 'Small']\n",
        "    is_keras_model = (model_type == 'NN_Arch1' or model_type == 'NN_Arch2')\n",
        "\n",
        "    for train_tier in tier_names:\n",
        "        # Get the model trained on the composite tier data\n",
        "        model_name = f'{train_tier}_Composite'\n",
        "        trained_model = composite_models.get(model_name, {}).get(model_type)\n",
        "\n",
        "        if trained_model is None:\n",
        "            continue\n",
        "\n",
        "        for test_tier in tier_names:\n",
        "            if train_tier == test_tier:\n",
        "                continue # Skip self-comparison\n",
        "\n",
        "            test_data_key = f'{test_tier}_Composite'\n",
        "\n",
        "            # The test data used here must be the SCALED test set\n",
        "            test_data_entry = composite_test_data.get(test_data_key, {})\n",
        "            X_test_scaled = test_data_entry.get('X_test')\n",
        "            y_test = test_data_entry.get('y_test')\n",
        "\n",
        "            if X_test_scaled is None:\n",
        "                continue\n",
        "\n",
        "            # Evaluate the model on the cross-tier test data\n",
        "            results = evaluate_model(trained_model, X_test_scaled, y_test, is_keras=is_keras_model)\n",
        "\n",
        "            key = f'Trained on {train_tier} | Tested on {test_tier}'\n",
        "            cross_tier_results[key] = results\n",
        "\n",
        "    # Format the results into a readable DataFrame\n",
        "    if not cross_tier_results:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    final_df = pd.DataFrame(cross_tier_results).T\n",
        "    final_df = final_df.apply(lambda x: pd.Series(x).map(lambda y: f'{y:.4f}'))\n",
        "    return final_df\n",
        "\n",
        "#  EXECUTE ANALYSIS\n",
        "\n",
        "try:\n",
        "    cross_tier_results_df = perform_cross_tier_analysis(\n",
        "        composite_models,\n",
        "        composite_test_data,\n",
        "        model_type='NN_Arch2'\n",
        "    )\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"Phase 3 Complete: Cross-Tier NN Generalization Analysis (R-Squared)\")\n",
        "    print(\"=\"*70)\n",
        "    if not cross_tier_results_df.empty:\n",
        "        # Displaying only the R2 values for key insight\n",
        "        print(cross_tier_results_df['R2'].to_markdown(numalign=\"left\", stralign=\"left\"))\n",
        "    else:\n",
        "        print(\"Analysis failed. Please confirm Phase 2 executed successfully and variables are defined.\")\n",
        "\n",
        "except NameError as e:\n",
        "    print(f\"\\nERROR: A required variable is missing. Did you run the full Phase 2 code? Missing variable: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b978fba",
      "metadata": {
        "id": "1b978fba"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.11"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}